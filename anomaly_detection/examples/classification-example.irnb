{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"library(keras)\nlibrary(ggplot2)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:44:35.893672Z","iopub.execute_input":"2021-05-29T11:44:35.895497Z","iopub.status.idle":"2021-05-29T11:44:37.238103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1) Binary (2-class) classification","metadata":{}},{"cell_type":"code","source":"# Data (2 classes)\n\nset.seed(9)\ndat = rbind( MASS::mvrnorm(n=500, mu=c(3,3),   Sigma=matrix(c(2,-1/2,-1/2,2), 2,2), empirical=TRUE),\n             MASS::mvrnorm(n=500, mu=c(-3,-3), Sigma=matrix(c(2,1/2,1/2,2), 2,2), empirical=TRUE) )\ndat = data.frame(dat, rep(0:1, each=500))\nnames(dat) = c(\"x1\",\"x2\",\"y\")\nbatch = nrow(dat)     # Batch size is data size since all data is in memory\n\nggplot(dat, aes(x=x1, y=x2, col=as.factor(y))) +\n    geom_point(show.legend=FALSE)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:44:42.499336Z","iopub.execute_input":"2021-05-29T11:44:42.524451Z","iopub.status.idle":"2021-05-29T11:44:43.157213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1a) Binary classification as logistic regression\n\n* 1-unit output layer representing probability of class \"1\"\n* sigmoid activation\n* binary_crossentropy loss\n* Training output represented by numeric indicator\n* Decision rule accepts class \"1\" if p>0.5\n","metadata":{}},{"cell_type":"code","source":"# Binary classification as logistic regression\n\n# Training inputs and outputs\nX = as.matrix(dat[c(\"x1\",\"x2\")])      # Must be matrix, (not data frame). Usually you would scale to normalize inputs\nY = dat$y                             # Numeric indicator of output class\n\n# Model\nmodel = keras_model_sequential(list(\n          layer_input(shape=2),                           # Shape must match number of inputs\n          layer_dense(units=1, activation=\"sigmoid\") ))   # 1-unit output layer for regression\n\ncompile(model, optimizer=optimizer_adam(lr=0.01), loss=\"binary_crossentropy\")\nfit(model, x=X, y=Y, epochs=200, batch_size=batch, verbose=FALSE)\n\n# Confusion matrix\np = predict(model, X)        # Fitted values (outputs predicted from training inputs)\ntable(input=Y, output=round(p))  # Threshold probability at 0.5. Note round(0.5) == 0\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:44:51.581483Z","iopub.execute_input":"2021-05-29T11:44:51.582854Z","iopub.status.idle":"2021-05-29T11:44:59.07864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1b) Binary classification as 2-class multiple classification\n\n* 2-unit output layer representing probability of each mutually exclusive class\n* softmax activation\n* categorical_crossentropy loss\n* Training outputs represented by 2 one-hot coded dummy variables\n* Decision rule accepts output class with highest probability\n","metadata":{}},{"cell_type":"code","source":"# Binary classification as 2-class multiple classification\n\n# Training inputs and outputs\nX = as.matrix(dat[c(\"x1\",\"x2\")])          # Must be matrix, (not data frame). Usually you would scale to normalize inputs\nY = to_categorical(dat$y, num_classes=2)  # One-hot coded matrix (2 dummy variables). Numeric indicator y must start from 0\n\n# Model\nmodel = keras_model_sequential(list(\n          layer_input(shape=2),                           # Shape must match number of inputs\n          layer_dense(units=2, activation=\"softmax\") ))   # Output units must match number of output classes\n\ncompile(model, optimizer=optimizer_adam(lr=0.01), loss=\"categorical_crossentropy\")\nfit(model, x=X, y=Y, epochs=200, batch_size=batch, verbose=FALSE)\n\n# Confusion matrix\np = predict(model, X)        # Fitted values (outputs predicted from training inputs)\nm1 = apply(Y, 1, which.max)  # Input class membership\nm2 = apply(p, 1, which.max)  # Output class membership\ntable(input=m1, output=m2)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:45:07.717967Z","iopub.execute_input":"2021-05-29T11:45:07.719296Z","iopub.status.idle":"2021-05-29T11:45:08.753897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" \n 2) Multiple classification","metadata":{}},{"cell_type":"code","source":"# Data (3 classes)\n\nset.seed(9)\ndat = rbind( MASS::mvrnorm(n=500, mu=c(3,3),   Sigma=matrix(c(2,-1/2,-1/2,2), 2,2), empirical=TRUE),\n             MASS::mvrnorm(n=500, mu=c(-3,-3), Sigma=matrix(c(2,1/2,1/2,2), 2,2), empirical=TRUE),\n             MASS::mvrnorm(n=500, mu=c(3,-2), Sigma=matrix(c(2,1/2,1/2,2), 2,2), empirical=TRUE) )\ndat = data.frame(dat, rep(0:2, each=500))\nnames(dat) = c(\"x1\",\"x2\",\"y\")\nbatch = nrow(dat)     # Batch size is data size since all data is in memory\n\nggplot(dat, aes(x=x1, y=x2, col=as.factor(y))) +\n    geom_point(show.legend=FALSE)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:45:17.157494Z","iopub.execute_input":"2021-05-29T11:45:17.158719Z","iopub.status.idle":"2021-05-29T11:45:17.465784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Multiple classification\n\n# Training inputs and outputs\nX = as.matrix(dat[c(\"x1\",\"x2\")])          # Must be matrix, (not data frame). Usually you would scale to normalize inputs\nY = to_categorical(dat$y, num_classes=3)  # One-hot coded matrix (3 dummy variables). Numeric indicator y must start from 0\n\n# Model\nmodel = keras_model_sequential(list(\n          layer_input(shape=2),                           # Shape must match number of inputs\n          layer_dense(units=3, activation=\"softmax\") ))   # Units must match number of output classes\n\ncompile(model, optimizer=optimizer_adam(lr=0.01), loss=\"categorical_crossentropy\")\nfit(model, x=X, y=Y, epochs=200, batch_size=batch, verbose=FALSE)\n\n# Confusion matrix\np = predict(model, X)        # Fitted values (outputs predicted from training inputs)\nm1 = apply(Y, 1, which.max)  # Input class membership\nm2 = apply(p, 1, which.max)  # Output class membership\ntable(input=m1, output=m2)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T11:45:23.126765Z","iopub.execute_input":"2021-05-29T11:45:23.128095Z","iopub.status.idle":"2021-05-29T11:45:24.185808Z"},"trusted":true},"execution_count":null,"outputs":[]}]}